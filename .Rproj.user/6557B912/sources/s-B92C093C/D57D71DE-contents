#############################################################################
## Part One
#############################################################################
library(readxl)
library(ggplot2)
#install.packages("arsenal")
library(arsenal)

#read in the data and get a quick summary
desk = read_excel("Help Desk.xlsx"); desk
colnames(desk)[colnames(desk) == "Number of Calls"] <- "Number_of_Calls" #change column name to use with arsenal

#it looks like there are negative inbound calls, which can't be true. has to be > 0
summary(desk) 

#convert to absolute value, assuming the negative sign was an error
desk$Number_of_Calls = abs(desk$Number_of_Calls) 

#when I made the initial bar chart I noticed that there were 2 different Tuesdays
unique(desk$Day) 
desk$Day[desk$Day == "Tue"] = "Tues"

#factor Hour and Day to create better grouping for visualization
levels(desk$Hour)
levels(desk$Day)
desk$Hour = factor(desk$Hour,
                   c('midnight-2am',
                     '2am - 4am',
                     '4am - 6am',
                     '6am - 8am',
                     '8am - 10am',
                     '10am - noon',
                     'noon - 2pm',
                     '2pm - 4pm',
                     '4pm - 6pm',
                     '6pm - 8pm',
                     '8pm - 10pm',
                     '10pm - midnight')); levels(desk$Hour)
desk$Day = factor(desk$Day,
                  c('Mon','Tues','Wed','Thur','Fri')); levels(desk$Day)



my_controls = tableby.control(
  test = T,
  total = T,
  numeric.test = "kwt", cat.test = "chisq",
  numeric.stats = c("meansd", "median", "q1q3", "range", "Nmiss2"),
  cat.stats = c("countpct", "Nmiss2"),
  stats.labels = list(
    meansd = "Mean (SD)",
    median = "Median",
    #mode = "Mode",
    q1q3 = "(Q1, Q3)",
    range = "Min - Max",
    Nmiss2 = "Missing"
  )
)

my_labels = list(
  Hour = "Hour Period (2 hour increments)",
  Day = "Day of Week"
)

#get a summary by day
#tapply(desk$Number_of_Calls, desk$Day, summary)
by_day = tableby(Day ~ Number_of_Calls, data = desk, control = my_controls)
summ_by_day = as.data.frame(by_day); summ_by_day

#get a summary by hour
#tapply(desk$Number_of_Calls, desk$Hour, summary)
by_hour = tableby(Hour ~ Number_of_Calls, data = desk, control = my_controls)
summ_by_hour = as.data.frame(by_hour); summ_by_hour

library(psych)
psych::describe(desk)
psych::describeBy(desk, desk$Day)
psych::describeBy(desk, desk$Hour)


#correlation by day
cor_day = desk[ , c("Day","Number_of_Calls")]; cor_day
boxplot(Number_of_Calls ~ Day, data = cor_day) #box plot of calls by day
cor_day$Day  = as.integer(cor_day$Day) #convert to integer based on levels for correlation
cor(cor_day$Day, cor_day$Number_of_Calls) #-0.43

#correlation by hour
cor_hour = desk[ , c("Hour","Number_of_Calls")]; cor_hour
boxplot(Number_of_Calls ~ Hour, data = cor_hour) #box plot of calls by hour
#this boxplot is a really good representation of the grouping of calls by hour
#it shows that, other than monday, most 2 hour segments from 6pm - 4am are relatively tightly grouped
#Monday seems to appear as the upper outlier in almost every plot other than a couple
#the lower outliers seem to be Tuesday for the 2am-4am hour and Wednesday for the 2pm-4pm hour
#I was able to infer outliers by the barchart created at the end, which was the most helpful visualization
cor_hour$Hour  = as.integer(cor_hour$Hour) #convert to integer based on levels for correlation
cor(cor_hour$Hour, cor_hour$Number_of_Calls) #-0.166

#scatter plot matrix to visualize correlations
plot(desk) #this shows that the coorelation between day and calls above is misleading


#frequency and percentage table for day
freq_day = do.call(data.frame, 
                   aggregate(desk$Number_of_Calls, 
                             by=list(Day = desk$Day), 
                             FUN = sum)
                   ); freq_day
freq_day = freq_day[order(freq_day$Day),]
colnames(freq_day)[colnames(freq_day) == "x"] <- "Sum"
freq_day[ , "Pct"] = round(freq_day$Sum / sum(freq_day$Sum), digits = 3)
freq_day[ , "Cum_Sum"] <- cumsum(freq_day$Sum)
freq_day[ , "Cum_Pct"] <- round(freq_day$Cum_Sum / sum(freq_day$Sum), digits = 3); freq_day

#frequency and pertange table for hour
freq_hour = do.call(data.frame, 
                   aggregate(desk$Number_of_Calls, 
                             by=list(Hour=desk$Hour), 
                             FUN = sum)
                   )
freq_hour = freq_hour[order(freq_hour$Hour),]
colnames(freq_hour)[colnames(freq_hour) == "x"] <- "Sum"
freq_hour[ , "Pct"] = round(freq_hour$Sum / sum(freq_hour$Sum), digits = 3)
freq_hour[ , "Cum_Sum"] <- cumsum(freq_hour$Sum)
freq_hour[ , "Cum_Pct"] <- round(freq_hour$Cum_Sum / sum(freq_hour$Sum), digits = 3);

#frequency and pertange table for day and hour
freq_both = do.call(data.frame, 
                   aggregate(desk$Number_of_Calls, 
                             by=list(Day = desk$Day, Hour=desk$Hour), 
                             FUN = sum)
                   );
freq_both = freq_both[order(freq_both$Day),]
colnames(freq_both)[colnames(freq_both) == "x"] <- "Sum"
freq_both[ , "Pct"] = round(freq_both$Sum / sum(freq_both$Sum), digits = 3)
freq_both[ , "Cum_Sum"] <- cumsum(freq_both$Sum)
freq_both[ , "Cum_Pct"] <- round(freq_both$Cum_Sum / sum(freq_both$Sum), digits = 3);

freq_day
freq_hour
freq_both

#different approach to frequency table
library(dplyr)
freq = desk %>%
  group_by(Day, Hour) %>%
  arrange(Day, Hour) %>%
  summarise(Sum = sum(Number_of_Calls))%>%
  mutate(Pct = Sum / sum(Sum)) %>%
  mutate(Cum_Sum = cumsum(Sum)) %>%
  mutate(Cum_Pct = Cum_Sum / sum(Sum)); print(freq, n=60)
  

#bar chart by day and hour to see volume
#this is by far the best look at the data in aggregate
ggplot(desk, aes(x = Hour, y = Number_of_Calls, fill = Day)) +
  #geom_bar(fill = "#0073C2FF", stat = "identity", position=position_dodge()) +
  geom_bar(stat = "identity", position=position_dodge()) +
  theme(axis.title.x = element_text(face="bold", colour="#990000", size=8),
        axis.text.x  = element_text(angle=90, vjust=0.5, size=8))

#############################################################################
## Part Two
#############################################################################
#5.12 ###################################
#Calculating by hand since midterm will required by hand
#Expected Return
X_ret = (-100)*(0.1) + (0)*(0.3) + (80)*(0.3) + (150)*(0.3); X_ret
Y_ret = (50)*(0.1) + (150)*(0.3) + (-20)*(0.3) + (-100)*(0.3); Y_ret
#Standard Deviation
X_std = sqrt(
            ((-100) - X_ret)**2 + (0 - X_ret)**2 + (80 - X_ret)**2 + (150 - X_ret)**2
        ); X_std
Y_std = sqrt(
            (50 - Y_ret)**2 + (150 - Y_ret)**2 + ((-20) - Y_ret)**2 + ((-100) - Y_ret)**2
        ); Y_std
#Covariance
X_Y_cov = (
            ((-100) - X_ret)*(50 - Y_ret)*(0.1) 
            + (0 - X_ret)*(150 - Y_ret)*(0.3) 
            + (80 - X_ret)*((-20) - Y_ret)*(0.3) 
            + (150 - X_ret)*((-100) - Y_ret)*(0.3)
          ); X_Y_cov

#5.13 ####################################
P_ret.thirty = (0.3)*X_ret + (1 - 0.3)*Y_ret; P_ret.thirty
P_ret.fifty = (0.5)*X_ret + (1 - 0.5)*Y_ret; P_ret.fifty
P_ret.seventy = (0.7)*X_ret + (1 - 0.7)*Y_ret; P_ret.seventy

#Question 3 ##############################
#1
1 - pbinom(5, 10, 0.46) #cdf
#2
1 - pbinom(5, 10, 0.34) #cdf
#3
dbinom(0, 10, 1 - 0.46) #pdf

#Question 4 ##############################
#1
pnorm(321,396,50)
#2
pnorm(471,396,50)-pnorm(32,396,50)
#3
1-pnorm(471,396,50)
#4
qnorm(0.01,396,50)

#Question 5 ##############################
wait = read_excel("Wait.xlsx"); wait
hist(wait$Waiting)
qqnorm(wait$Waiting)
describe(wait$Waiting)
hist(wait$Seating)
qqnorm(wait$Seating)
describe(wait$Seating)

#Question 8 ##############################
pnorm(300000,270100,9000)
pnorm(290000,270100,9000)-pnorm(275000,270100,9000)

